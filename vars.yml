###############################################################################################
#                                      -General Options-                                      # 
###############################################################################################
# These parameters are required.                                                              #
###############################################################################################

# qcow2 image to use for your Virtual Machines: 
qcow2_image: 'https://cdimage.debian.org/cdimage/openstack/current-10/debian-10-openstack-amd64.qcow2'

# Location to use for storing the qcow2 image. Be sure to end the absolute path with a '/'.
qcow2_download_location: '/tmp/'

# Resource Pool for your virtual machines.
k8s_resource_pool: 'Kubernetes'

# Path to the SSH Public Key on your Proxmox Server.
k8s_ssh_key: '/root/.ssh/id_rsa.pub'

# VM IDs
k8s_master_id: '3010'
k8s_node1_id: '30100'
k8s_node2_id: '30101'
k8s_node3_id: '30102'

# Hostnames
k8s_master_hn: 'Elpatron'
k8s_node1_hn: 'JavierPena'
k8s_node2_hn: 'SteveMurphy'
k8s_node3_hn: 'LaQuica	'

# Number of CPUs
k8s_master_cpu: '2'
k8s_node1_cpu: '4'
k8s_node2_cpu: '4'
k8s_node3_cpu: '4'

# Amount of memory expressed in megabytes
k8s_master_mem: '5120'
k8s_node1_mem: '15360'
k8s_node2_mem: '15360'
k8s_node3_mem: '15360'

# Disk Sizes
k8s_master_size: '50G'
k8s_node1_size: '50G'
k8s_node2_size: '50G'
k8s_node3_size: '50G'

# IP Addresses
k8s_master_ip: '51.77.197.180'
k8s_node1_ip: '51.77.197.181'
k8s_node2_ip: '51.77.197.182'
k8s_node3_ip: '51.77.197.183'

# Subnet Sizes
k8s_master_sn: '/28'
k8s_node1_sn: '/28'
k8s_node2_sn: '/28'
k8s_node3_sn: '/28'

# Network Gateway
k8s_master_gw: '51.77.197.190'
k8s_node1_gw: '51.77.197.190'
k8s_node2_gw: '51.77.197.190'
k8s_node3_gw: '51.77.197.190'

# DNS Servers
k8s_master_ns: '8.8.8.8'
k8s_node1_ns: '8.8.8.8'
k8s_node2_ns: '8.8.8.8'
k8s_node3_ns: '8.8.8.8'

# Search Domains
k8s_master_sd: 'master.nan.ci'
k8s_node1_sd: 'node1.nan.ci'
k8s_node2_sd: 'node2.nan.ci'
k8s_node3_sd: 'node3.nan.ci'

# Network Bridges
k8s_master_bridge: 'vmbr0'
k8s_node1_bridge: 'vmbr0'
k8s_node2_bridge: 'vmbr0'
k8s_node3_bridge: 'vmbr0'

# Storage volumes
k8s_master_stg: 'SaturnPool'
k8s_node1_stg: 'SaturnPool'
k8s_node2_stg: 'SaturnPool'
k8s_node3_stg: 'SaturnPool'

# CIDR for the Calico Pod Network - MUST be different from your Kubernetes CIDR to avoid an IP conflict. Subnet size will automatically be set to /16.
calico_cidr: '172.17.0.0'

# URL for the Calico Network Policy: 
# Can be found here: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tabs-pod-install-2
calico_policy_url: 'https://docs.projectcalico.org/v3.8/manifests/calico.yaml'

# The URL for the Kubernetes Dashboard Manifest. Not locally hosted as it changes frequently.
# Can be found in the `Getting Started` section of this page: https://github.com/kubernetes/dashboard/blob/master/README.md
k8s_dasboard_url: 'https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml'

# Service user created for the dashboard.
k8s_user_name: 'keffaService'


###############################################################################################
#                                        -VLAN Options-                                       # 
###############################################################################################
# These parameters are only required if your network is configured with VLANs and you wish to #
# specify which VLAN each of your Kubernetes nodes is allocated to. Leave these variables     #
# blank if you do not intend to use VLANs for your cluster.                                   #
###############################################################################################

# VLAN Tags
k8s_master_vlan: '40'
k8s_node1_vlan: '40'
k8s_node2_vlan: '40'
k8s_node3_vlan: '40'


###############################################################################################
#                                    -Unattended Upgrades-                                    # 
###############################################################################################
# These parameters are only required if you want to enable Unattended Upgrades on the         #
# operating systems that comprise your Kubernetes master and worker nodes. None of these      #
# variables are required to enable the feature. However, you may wish to enable email         #
# notifications via SMTP or tweak the way in which Unattended Upgrades are applied to your    #
# system. To do so, simply uncomment the relevant configuration setting.                      #
#                                                                                             #
# NOTE: If you enable any of the following variables, then you must enable the other ones as  #
# well or SMTP will not work:                                                                 #
#     - smtp_server                                                                           #
#     - smtp_port                                                                             #
#     - smtp_username                                                                         #
#     - smtp_password                                                                         #
#     - email_address                                                                         #
###############################################################################################

# SMTP Server
smtp_server: 'cwp.nan.ci'

# SMTP Port 
smtp_port: '465'

# SMTP Server username
smtp_username: 'keffa@nan.ci'

# SMTP Server password
smtp_password: '3fdfca49'

# The email address to which notifications will be delivered.
email_address: 'keffa@nan.ci'

# You want to be naughty and use SMTP without TLS.
#smtp_use_tls: 'off'

# Only receive emails when an error occurs.
mail_only_on_error: "true"

# Only perform upgrades when a graceful OS shutdown occurs.
#update_only_on_shutdown: "true"

# Automatically remove unused kernel packages.
remove_unused_kernel_packages: "true"

# Automatically remove unused dependencies. 
remove_unused_dependencies: "true"

# Automatically perform a reboot after upgrading if required.
#automatic_reboot_if_required: "true"

# If automatic reboots are enabled, configure the time at which they occur.
#automatic_reboot_time: "02:00"

# Enable logging to syslog.
#enable_syslog_logging: "true"


###############################################################################################
#                                        -NFS Options-                                        # 
###############################################################################################
# These parameters are only required if you have an NFS server and would like to enable the   #
# optional support for dynamic provisioning of Persistent Storage volumes for your pods.      #
###############################################################################################

# NFS server
nfs_hostname: 'saturn.sol.milkyway'

# NFS Mount Point
nfs_mount_point: '/SaturnPool/Kubernetes'

# NFS Provisioner Name
nfs_provisioner: 'saturnpool'

# Namespace to deploy the NFS Provisioner to.
nfs_namespace: 'nfs-provisioner'

# Default reclaim policy. https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy
nfs_reclaim_policy: 'Retain'


###############################################################################################
#                                      -MetalLB Options-                                      # 
###############################################################################################
#                                                                                             #
# WARNING: MetalLB is currently supported, however dynamic provisioning is NOT currently      #
# supported despite the explanation below. This will come in a later release, however, if you #
# are interested in using this feature you will need to modify the configmap located in the   #
# MetalLB files directory yourself to match the variables you declare here.                   #
#                                                                                             #
# These parameters are only required if you want to enable on-prem load balancing with        #
# MetalLB. The MetalLB configuration file will be dynamically provisioned based on how many   #
# variable sets you declare below. This is intended to allow you to provision a single or     #
# multiple address pools depending on your personal network configuration. The configuration  #
# is dynamically generated based on how many sets of variables you list below. For example:   #
#                                                                                             #
# If you wish to deploy a single address pool, you would declare these three variables.       #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
#                                                                                             #
#                                                                                             #
# However, if you wished to declare two address pools, you would include a second set of      #
# variables where the `ap1` portion of the variable name is incremeneted by one.              #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap2_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap2_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
# metallb_ap2_cidr: CIDR                                                                      #
###############################################################################################

# The IP Address of the router you wish to peer with.
metallb_peer_router: '192.168.1.1'

# The AS Number of the router you wish to peer with.
metallb_peer_asn: '64512'

# The names of your address pools.
metallb_ap1_name: 'VLAN10'
metallb_ap4_name: 'VLAN70'
metallb_ap2_name: 'VLAN50'
metallb_ap3_name: 'VLAN60'

# The protocols of your address pools. (bgp|layer2)
metallb_ap1_protocol: 'bgp'
metallb_ap2_protocol: 'bgp'
metallb_ap3_protocol: 'bgp'
metallb_ap4_protocol: 'bgp'

# The CIDRs of your address pools.
metallb_ap1_cidr: '192.168.10.1/24'
metallb_ap2_cidr: '192.168.50.1/24'
metallb_ap3_cidr: '192.168.60.1/24'
metallb_ap4_cidr: '192.168.70.1/24'


###############################################################################################
#                              -NGINX Ingress Controller Options-                             # 
###############################################################################################
# These parameters are only required if you want to enable the NGINX ingress controller.      #
# REQUIREMENT: The NGINX Ingress controller integration requires a Load Balancer integration. #
###############################################################################################

# The IP Address to use 
nginx_load_balancer_ip: '192.168.50.100'


###############################################################################################
#                                      -DataDog Options-                                      # 
###############################################################################################
# These parameters are only required if you have an account with DataDog and would like to    #
# enable metrics collection for your cluster.                                                 #
###############################################################################################

# Your DataDog API Key
dd_api_key: 'APIKEY'

# Namespace to deploy DataDog to
dd_namespace: 'default'
